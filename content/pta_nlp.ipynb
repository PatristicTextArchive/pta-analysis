{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query PTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTA ad plaintext data\n",
    "\n",
    "(This was generated with another script and exported to a CSV file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/severian_plaintext.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = df.loc[df['urn'] == 'pta0001.pta017.pta-grc1', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = \"\".join(select.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary queries with the [Classical Language Dictionary (CLD)](https://cld.bbaw.de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_wrapper(lemma,lang):\n",
    "    URL = \"https://cld.bbaw.de/api/dictionary/lemma/\"+lemma\n",
    "    PARAMS = {'language': lang, 'options': 'regex'}\n",
    "    r = requests.get(url = URL, params = PARAMS)\n",
    "    data = r.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictentry= dictionary_wrapper(\"ἀδελφός\", \"grc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictentry[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dictionary in dictentry[\"data\"][0][\"descriptions\"]:\n",
    "    print(dictionary[\"dictionary\"])\n",
    "    display(HTML(dictionary[\"description\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse a text with the help of the [Classical Language Dictionary (CLD)](https://cld.bbaw.de)\n",
    "\n",
    "Disclaimer: Currently the CLD only allows a maximum length of 1500 characters per query. So, we need to use some workarounds. CLD currently also does use outdated versions of the spacy models, so there are more errors. And CLD does currently not give access to all information provided by the spacy model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_text(text, max_length=1000):\n",
    "    slices = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        \n",
    "        # If the end exceeds the text length, set it to the text length\n",
    "        if end >= len(text):\n",
    "            slices.append(text[start:])\n",
    "            break\n",
    "        \n",
    "        # Find the last punctuation mark before the end limit\n",
    "        last_punctuation = max(text.rfind(p, start, end) for p in '·;.')\n",
    "\n",
    "        # If no punctuation is found, we have to slice at max_length\n",
    "        if last_punctuation == -1:\n",
    "            last_punctuation = end\n",
    "        \n",
    "        # Append the slice and update the start position\n",
    "        slices.append(text[start:last_punctuation + 1])  # Include the punctuation\n",
    "        start = last_punctuation + 1  # Move past the punctuation\n",
    "\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_wrapper(text):\n",
    "    URL = \"https://cld.bbaw.de/api/analyze\"\n",
    "    # max char length of 1500\n",
    "    # defining a params dict for the parameters to be sent to the API, all parameters: https://cld.bbaw.de/api/analyze/parameters\n",
    "    PARAMS = {'text':text, 'model':'grc_proiel_lg', 'attributes': 'lemma_|pos_|tag_|ent_type_|dep_|morph|dep_|head|is_stop|is_punct|is_sent_start|is_sent_end|vector', 'output':'list'}\n",
    "    r = requests.post(url = URL, params = PARAMS)\n",
    "    data = r.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = slice_text(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for entry in slices:\n",
    "    analysis = analyse_wrapper(entry)\n",
    "    doc.extend(analysis[\"data\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed = pd.DataFrame(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.width', None)\n",
    "display(analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple questions answered using the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the most used words in the text?\n",
    "\n",
    "without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token[\"lemma\"]\n",
    "        for token in doc\n",
    "        if not token[\"is_stop\"] and not token[\"is_punct\"]]\n",
    "\n",
    "word_counts = Counter(words)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, font_path='data/DejaVuSans.ttf').generate_from_frequencies(dict(word_counts))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words = word_counts.most_common(20)\n",
    "words, frequencies = zip(*most_frequent_words)\n",
    "\n",
    "# Plot the most frequent words\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, frequencies)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('20 most Frequent Words')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add the number of mentions to each bar\n",
    "for i, freq in enumerate(frequencies):\n",
    "    plt.text(i, freq + 1, str(freq), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the named entities in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token[\"ent_type\"]:\n",
    "        print(token[\"text\"],token[\"ent_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which are the 10 most used verbs, nouns and adjectives? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "verbs = []\n",
    "for token in doc:\n",
    "    if not token[\"is_stop\"] and token[\"pos\"] == \"NOUN\":\n",
    "        nouns.append(token[\"lemma\"])\n",
    "    if not token[\"is_stop\"] and token[\"pos\"] == \"ADJ\":\n",
    "        adjectives.append(token[\"lemma\"])\n",
    "    if not token[\"is_stop\"]  and token[\"pos\"] == \"VERB\":\n",
    "        verbs.append(token[\"lemma\"])\n",
    "\n",
    "print(Counter(verbs).most_common(10))\n",
    "print(Counter(nouns).most_common(10))\n",
    "print(Counter(adjectives).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What combinations of substantives and adjectives are in the text?\n",
    "\n",
    "For the abbreviations used for the dependency cf. https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token.dep_ = amod und passendes token.head.i bzw. token.head.text\n",
    "pairs = []\n",
    "for token in doc: \n",
    "    if not token[\"is_punct\"]:\n",
    "        if token[\"dep\"] == \"amod\":\n",
    "            #print(doc[token.i].similarity(doc[token.head.i]))\n",
    "            pairs.append(token[\"text\"]+\" \"+token[\"head\"])\n",
    "counter = Counter(pairs)\n",
    "for element, count in counter.most_common():\n",
    "    print(f\"{element:<25}: {count}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the text say about Jews and heretics?\n",
    "\n",
    "We want to get all instances when Jew or heretic is in the nominative; we also want to get the verb that is used to describe their action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jews and Haeretics\n",
    "for token in doc:\n",
    "    if token[\"lemma\"] == \"Ἰουδαῖος\" and \"Case=Nom\" in token[\"morph\"]: #and token.dep_ == \"nsubj\":\n",
    "        print(token[\"text\"], token[\"head\"])\n",
    "    if token[\"lemma\"] == \"αἱρετικός\" and \"Case=Nom\" in token[\"morph\"]:\n",
    "        print(token[\"text\"], token[\"head\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
